{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codjere/RESEARCHMETHOD-Data/blob/main/Datalama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-mBK4UE2lV3",
        "outputId": "4cae3b9b-6423-458d-e0e8-2a4ee068f01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data sebelum preprocessing: 1066\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 1) IMPORT LIBRARY & LOAD DATA\n",
        "# ================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, re, glob\n",
        "\n",
        "# CSV files\n",
        "csv_files = [\n",
        "    \"/content/data_tiktok_1.csv\",\n",
        "    \"/content/data_tiktok_2.csv\",\n",
        "    \"/content/data_tiktok_3.csv\",\n",
        "    \"/content/data_tiktok_4.csv\",\n",
        "    \"/content/data_tiktok_5.csv\"\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "for file in csv_files:\n",
        "    try:\n",
        "        df_tmp = pd.read_csv(file, encoding=\"utf-8\")\n",
        "    except:\n",
        "        df_tmp = pd.read_csv(file, encoding=\"latin-1\")\n",
        "    df_tmp[\"source_file\"] = os.path.basename(file)\n",
        "    dfs.append(df_tmp)\n",
        "\n",
        "raw_df = pd.concat(dfs, ignore_index=True)\n",
        "print(\"Jumlah data sebelum preprocessing:\", len(raw_df))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 2) NORMALISASI KOLOM\n",
        "# ================================\n",
        "rename_map = {\n",
        "    \"videoWebUrl\": \"video_url\",\n",
        "    \"createTimeISO\": \"created_at\",\n",
        "    \"text\": \"text\",\n",
        "    \"uniqueId\": \"username\",\n",
        "    \"uid\": \"user_id\",\n",
        "    \"diggCount\": \"like_count\",\n",
        "    \"replyCommentTotal\": \"reply_count\",\n",
        "    \"likedByAuthor\": \"liked_by_author\",\n",
        "}\n",
        "\n",
        "for old, new in rename_map.items():\n",
        "    if old in raw_df.columns:\n",
        "        raw_df = raw_df.rename(columns={old: new})\n",
        "\n",
        "print(\"Kolom setelah normalisasi:\", raw_df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYethIxL3LWJ",
        "outputId": "901f6461-26dc-4b9e-9563-648910433bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kolom setelah normalisasi: ['video_url', 'submittedVideoUrl', 'input', 'cid', 'createTime', 'created_at', 'text', 'like_count', 'liked_by_author', 'pinnedByAuthor', 'repliesToId', 'reply_count', 'user_id', 'username', 'avatarThumbnail', 'mentions', 'detailedMentions', 'source_file']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 3) PILIH KOLOM PENTING\n",
        "# ================================\n",
        "important_cols = [\n",
        "    \"video_url\", \"created_at\", \"text\", \"username\", \"user_id\",\n",
        "    \"like_count\", \"reply_count\", \"liked_by_author\"\n",
        "]\n",
        "important_cols = [c for c in important_cols if c in raw_df.columns]\n",
        "df = raw_df[important_cols].copy()\n",
        "print(\"Kolom yang dipakai:\", df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3sZNog43Ohw",
        "outputId": "d52aa831-04fd-44d0-e587-29d4402fbe05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kolom yang dipakai: ['video_url', 'created_at', 'text', 'username', 'user_id', 'like_count', 'reply_count', 'liked_by_author']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 4) CLEANING TEXT\n",
        "# ================================\n",
        "url_pat = re.compile(r\"http\\S+\")\n",
        "mention_pat = re.compile(r\"@\\w+\")\n",
        "ws_pat = re.compile(r\"\\s+\")\n",
        "\n",
        "def clean_text(s):\n",
        "    s = str(s)\n",
        "    s = url_pat.sub(\"\", s)                # hapus URL\n",
        "    s = mention_pat.sub(\"@user\", s)      # normalisasi mention\n",
        "    s = ws_pat.sub(\" \", s).strip()       # hapus spasi ganda\n",
        "    return s\n",
        "\n",
        "# 1) Cleaning awal: URL, mention, spasi ganda\n",
        "df[\"text_clean\"] = df[\"text\"].fillna(\"\").map(clean_text)"
      ],
      "metadata": {
        "id": "PIunycXz3Qpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 5) PARSE WAKTU & HAPUS DUPLIKAT\n",
        "# ================================\n",
        "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\")\n",
        "df[\"username\"] = df[\"username\"].astype(str).str.strip()\n",
        "\n",
        "df = df.drop_duplicates(\n",
        "    subset=[\"username\",\"text_clean\"],\n",
        "    keep=\"first\"\n",
        ")\n",
        "print(\"Jumlah data bersih:\", len(df))\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "df.to_csv(\"data/tiktok_dataset_clean.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdaAAm4_4dAy",
        "outputId": "2d5a4775-87bf-4208-90fd-c0b44f9a5a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data bersih: 1058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 6) DATA UNTUK SENTIMENT LABELING (TANPA SAMPLING)\n",
        "# ================================\n",
        "df_sent_base = df[df[\"text_clean\"].str.strip() != \"\"].copy()\n",
        "\n",
        "N_SENTIMENT = 400\n",
        "sent_sample = df_sent_base.sample(\n",
        "    n=min(N_SENTIMENT, len(df_sent_base)),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "sent_for_label = sent_sample[\n",
        "    [\"created_at\",\"username\",\"text_clean\",\n",
        "     \"like_count\",\"reply_count\",\"video_url\"]\n",
        "].reset_index(drop=True)\n",
        "\n",
        "sent_for_label.insert(0,\"id\",range(1,len(sent_for_label)+1))\n",
        "sent_for_label[\"sentiment_label\"] = \"\"\n",
        "\n",
        "# ðŸ”§ FIX EXCEL (INI WAJIB)\n",
        "sent_for_label[\"created_at\"] = (\n",
        "    pd.to_datetime(sent_for_label[\"created_at\"], errors=\"coerce\")\n",
        "    .dt.tz_localize(None)\n",
        ")\n",
        "\n",
        "# SIMPAN DATA\n",
        "os.makedirs(\"labeling\", exist_ok=True)\n",
        "sent_for_label.to_csv(\n",
        "    \"labeling/sentiment_labels_manual.csv\",\n",
        "    index=False\n",
        ")\n",
        "sent_for_label.to_excel(\n",
        "    \"labeling/sentiment_labels_manual.xlsx\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"Komentar untuk sentiment labeling:\", len(sent_for_label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyNrEWtx4e-A",
        "outputId": "da5c28bb-1eda-4b4e-b000-189668297e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Komentar untuk sentiment labeling: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 7) SAMPLING UNTUK BUZZER LABELING (MANUAL)\n",
        "# ================================\n",
        "df_user_base = df[df[\"username\"]!=\"\"].copy()\n",
        "\n",
        "user_counts = (\n",
        "    df_user_base\n",
        "    .groupby(\"username\", as_index=False)\n",
        "    .agg(total_comments=(\"text_clean\",\"count\"))\n",
        ")\n",
        "\n",
        "N_USERS = 250\n",
        "user_sample = (\n",
        "    user_counts\n",
        "    .sort_values(\"total_comments\", ascending=False)\n",
        "    .head(N_USERS)\n",
        ")\n",
        "\n",
        "rows = []\n",
        "for _, r in user_sample.iterrows():\n",
        "    uname = r[\"username\"]\n",
        "    sub = df_user_base[df_user_base[\"username\"]==uname]\n",
        "    sample_comment = sub.sort_values(\"created_at\").iloc[0][\"text_clean\"]\n",
        "\n",
        "    rows.append({\n",
        "        \"username\": uname,\n",
        "        \"total_comments\": int(r[\"total_comments\"]),\n",
        "        \"sample_comment\": sample_comment\n",
        "    })\n",
        "\n",
        "buzzer_for_label = pd.DataFrame(rows)\n",
        "buzzer_for_label[\"buzzer_label\"] = \"\"\n",
        "\n",
        "buzzer_for_label.to_csv(\n",
        "    \"labeling/buzzer_labels_manual.csv\",\n",
        "    index=False\n",
        ")\n",
        "buzzer_for_label.to_excel(\n",
        "    \"labeling/buzzer_labels_manual.xlsx\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"Akun untuk buzzer labeling:\", len(buzzer_for_label))"
      ],
      "metadata": {
        "id": "8nJJ74eV4lIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb662399-c598-4d0c-c289-c4fd89814449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akun untuk buzzer labeling: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 8) LOAD MANUAL SENTIMENT & BUZZER\n",
        "# ================================\n",
        "df = pd.read_csv(\"data/tiktok_dataset_clean.csv\")\n",
        "sentiment_manual = pd.read_csv(\n",
        "    \"labeling/sentiment_labels_manual.csv\"\n",
        ")\n",
        "sentiment_manual = sentiment_manual.drop(\n",
        "    columns=[\"Unnamed: 8\"], errors=\"ignore\"\n",
        ")\n",
        "\n",
        "df = df.merge(\n",
        "    sentiment_manual[[\"text_clean\",\"sentiment_label\"]],\n",
        "    on=\"text_clean\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "buzzer_manual = pd.read_csv(\"labeling/buzzer_labels_manual.csv\")\n",
        "df = df.merge(\n",
        "    buzzer_manual[[\"username\",\"buzzer_label\"]],\n",
        "    on=\"username\",\n",
        "    how=\"left\"\n",
        ")"
      ],
      "metadata": {
        "id": "yuy_DeF24oJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 9) FEATURE ENGINEERING BUZZER (XGBOOST)\n",
        "# ================================\n",
        "# comment_count, sentiment_mean, max_duplicate, length_mean\n",
        "freq = df.groupby('username').size().reset_index(name='comment_count')\n",
        "df = df.merge(freq, on='username', how='left')\n",
        "\n",
        "sentiment_map = {\"negative\":-1,\"neutral\":0,\"positive\":1}\n",
        "df['sentiment_num'] = df['sentiment_label'].map(sentiment_map)\n",
        "\n",
        "sent_avg = df.groupby('username')['sentiment_num'].mean().reset_index(name='sentiment_mean')\n",
        "df = df.merge(sent_avg, on='username', how='left')\n",
        "\n",
        "dup = df.groupby(['username','text_clean']).size().reset_index(name='duplicate_count')\n",
        "dup_user = dup.groupby('username')['duplicate_count'].max().reset_index(name='max_duplicate')\n",
        "df = df.merge(dup_user, on='username', how='left')\n",
        "\n",
        "df['comment_length'] = df['text_clean'].astype(str).apply(len)\n",
        "length_avg = df.groupby('username')['comment_length'].mean().reset_index(name='length_mean')\n",
        "df = df.merge(length_avg, on='username', how='left')\n",
        "\n",
        "df_user = df.groupby('username').agg({\n",
        "    'comment_count':'first',\n",
        "    'sentiment_mean':'first',\n",
        "    'max_duplicate':'first',\n",
        "    'length_mean':'first',\n",
        "    'buzzer_label':'first'\n",
        "}).reset_index()\n",
        "df_user = df_user[df_user['buzzer_label'].notna()]\n",
        "df_user['buzzer_label'] = df_user['buzzer_label'].astype(int)\n",
        "\n",
        "print(\"Distribusi kelas (persentase) sebelum balancing:\")\n",
        "print(df_user['buzzer_label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sjlz-vl4rIb",
        "outputId": "4201d50f-51a3-4f1d-a20b-a85295dd1a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi kelas (persentase) sebelum balancing:\n",
            "Series([], Name: proportion, dtype: float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA FINAL USER\n",
        "\n",
        "df_user = (\n",
        "    df.groupby(\"username\")\n",
        "    .agg({\n",
        "        \"comment_count\":\"first\",\n",
        "        \"sentiment_mean\":\"first\",\n",
        "        \"max_duplicate\":\"first\",\n",
        "        \"length_mean\":\"first\",\n",
        "        \"buzzer_label\":\"first\"\n",
        "    })\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "df_user = df_user[df_user[\"buzzer_label\"].notna()]\n",
        "df_user[\"buzzer_label\"] = df_user[\"buzzer_label\"].astype(int)\n",
        "\n",
        "print(\"Distribusi kelas:\")\n",
        "print(df_user[\"buzzer_label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qrVeUtv9_Od",
        "outputId": "82d7ac4f-8fc5-4e37-8cfa-1608ae99e9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi kelas:\n",
            "Series([], Name: count, dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_user shape:\", df_user.shape)\n",
        "print(df_user[\"buzzer_label\"].value_counts(dropna=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEiNssAYFr20",
        "outputId": "e49df5be-1d2b-45f1-ce9a-14beee288655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_user shape: (0, 6)\n",
            "Series([], Name: count, dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 10) SPLIT DATA\n",
        "# ================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_user[\n",
        "    [\"comment_count\",\"sentiment_mean\",\n",
        "     \"max_duplicate\",\"length_mean\"]\n",
        "]\n",
        "y = df_user[\"buzzer_label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train:\", X_train.shape)\n",
        "print(\"Test :\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "RYHLCDlx4wWz",
        "outputId": "9d87763f-9d7f-4506-df83-99a40a9202d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1818036022.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"buzzer_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 11) MODELING XGBOOST (3 VARIANTS)\n",
        "# ================================\n",
        "import xgboost as xgb\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "dx9B3ewK40E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- a) XGBoost Default ---\n",
        "xgb_base = xgb.XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "xgb_base.fit(X_train, y_train)\n",
        "pred_base = xgb_base.predict(X_test)\n",
        "print(\"=== XGBoost Default ===\")\n",
        "print(classification_report(y_test, pred_base, digits=3))"
      ],
      "metadata": {
        "id": "W9JFD-st42ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- b) XGBoost + Class Weight ---\n",
        "neg = (y_train==0).sum()\n",
        "pos = (y_train==1).sum()\n",
        "scale_pos = neg/pos if pos>0 else 1.0\n",
        "xgb_weight = xgb.XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric=\"logloss\",\n",
        "    scale_pos_weight=scale_pos,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_weight.fit(X_train, y_train)\n",
        "pred_weight = xgb_weight.predict(X_test)\n",
        "print(\"=== XGBoost + Class Weight ===\")\n",
        "print(classification_report(y_test, pred_weight, digits=3))"
      ],
      "metadata": {
        "id": "V8WHVFo845L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- c) XGBoost + SMOTE ---\n",
        "sm = SMOTE(random_state=42, k_neighbors=3)\n",
        "xgb_smote = xgb.XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "pipe_smote = Pipeline(steps=[('scaler',StandardScaler()),('smote',sm),('clf',xgb_smote)])\n",
        "pipe_smote.fit(X_train, y_train)\n",
        "pred_smote = pipe_smote.predict(X_test)\n",
        "print(\"=== XGBoost + SMOTE ===\")\n",
        "print(classification_report(y_test, pred_smote, digits=3))"
      ],
      "metadata": {
        "id": "zWHu6mEx470Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 12) FEATURE IMPORTANCE\n",
        "# ================================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "importance_dict = xgb_weight.get_booster().get_score(importance_type=\"gain\")\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": importance_dict.keys(),\n",
        "    \"Importance\": importance_dict.values()\n",
        "}).sort_values(by=\"Importance\", ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10,max(6,len(importance_df)*0.3)))\n",
        "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
        "plt.xlabel(\"Feature Importance (Gain)\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"XGBoost Feature Importance (Weighted Class)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SK_qW_nS4-mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 13) KLASIFIKASI BUZZER PAKAI INDOBERTWEET\n",
        "# ================================\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Gabungkan semua komentar per akun\n",
        "df_text_user = df.groupby('username')['text_clean'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "df_text_user = df_text_user.merge(df_user[['username','buzzer_label']], on='username', how='left')\n",
        "df_text_user = df_text_user[df_text_user['buzzer_label'].notna()]\n",
        "\n",
        "# Split train/test\n",
        "X_text = df_text_user['text_clean'].tolist()\n",
        "y_text = df_text_user['buzzer_label'].tolist()\n",
        "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
        "    X_text, y_text, test_size=0.2, random_state=42, stratify=y_text\n",
        ")\n",
        "\n",
        "MODEL = \"indolem/indobertweet-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
        "\n",
        "class BuzzerDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = tokenizer(self.texts[idx], truncation=True, max_length=128, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        item = {k:v.squeeze(0) for k,v in encoded.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = BuzzerDataset(X_train_text, y_train_text)\n",
        "test_dataset = BuzzerDataset(X_test_text, y_test_text)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(\"=== IndoBERTweet Classification Buzzer ===\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "Ul1XyTPv5Ces"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}